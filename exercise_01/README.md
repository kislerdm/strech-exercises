# JOIN tables using std python libraries

## Problem

As a stakeholder, I want to receive a report about non-blocked transactions in terms of cumulative transactions amount
and total number of unique active users broken down by category.

Unfortunately the query planner of our database can not optimize the query well enough in order to get the results.

## Proposed solution

To build a python application to join two tables, `users` and `transactions`. See the tables [DDL](#ddl) below.

### DDL

```sql
CREATE TABLE transactions
(
    transaction_id          UUID,
    date                    DATE,
    user_id                 UUID,
    is_blocked              BOOL,
    transaction_amount      INTEGER,
    transaction_category_id INTEGER
);

CREATE TABLE users
(
    user_id   UUID,
    is_active BOOLEAN
);
```

### Query

```sql
SELECT t.transaction_category_id,
       SUM(t.transaction_amount) AS sum_amount,
       COUNT(DISTINCT t.user_id) AS num_users
FROM transactions t
         JOIN users u USING (user_id)
WHERE t.is_blocked = False
  AND u.is_active = 1
GROUP BY t.transaction_category_id
ORDER BY sum_amount DESC;
```

## Acceptance Criteria

- Python application is developed to execute [the query](#query) operations and print result to _stdout_;
- The application satisfies the following success criteria:
    - [Correctness](#correctness-check)
    - [Efficiency](#efficiency)
    - [Code quality](#code-quality)
- Only standard python libraries are used;
- The solution does not concern SQL parsing;
- The solution is not generic, it does only concern this particular problem in its exact statement.

### Correctness check

Data quality assurance is performed according to the following test scenario using reference data fixtures.

**GIVEN**

- The inputs `users.csv` and `transactions.csv` generated using [the script](generate_data.py)

_Note_: The original script to generate the inputs was modified by fixing the random seed to "2022-11-02".

**WHEN**

- Developed application is executed: read the generated csv files and applies logic to generate the result.

**THEN**

- The program output matches the reference generated by running the [query](#query) on the input data sets uploaded to
  Postgres.

### Efficiency

Metrics:

- Execution time
- Memory allocation

_References_:
- https://wiki.python.org/moin/TimeComplexity
- https://realpython.com/sorting-algorithms-python/

### Code quality

Linting applied:

- black
- flak8
- mypy
- isort

## Solution

### Product Questions

- Does input's validation required?
    - The solution is being delivered under the assumption of positive answer. The reason: data quality for analytics is
      more critical than performance.

### Tech Questions and Decisions

- How to store user_id and transaction_id in memory?
    - UUID vs. str: what leads to higher memory allocation?
- When reading file, is `csv` library more efficient than line-by-line reading using standard file reader with `open`?
- Which sorting algorithm would suffice the cardinality of the problem?
    - Costs-benefit tradeoff: delivery effectiveness vs. technical efficiency, i.e. development complexity vs.
      time-complexity gain.
      <br>_Hypothesis_: the standard `sort` would be as efficient as it is feasibly possible because the data
      cardinality, or the number of sorted array's elements does not exceed a dozen. The hypothesis is based on personal
      user experience, provided fixtures generation [script](generate_data.py)
      and [the article](https://realpython.com/sorting-algorithms-python/) illustrating different
      sorting algorithms implementation in python.

### Execution path

1. Store the ID of _active_ users in memory as a [`Set`](https://docs.python.org/3/tutorial/datastructures.html#sets):
    - Benefits:
        - Minimisation of memory allocation, i.e. only the smaller dataset is stored in memory in full.
    - Requirements:
        - `WHERE` clause condition to be applied in-flight: filtering for `is_active` when reading the file
          line-by-line.
        - The array of `user_id` is fully stored in memory to realise the join condition.
2. Extract _relevant_ attributes of non-blocked transactions done _active_ users in memory:
    - Benefits:
        - Minimisation of memory allocation
    - Requirements:
        - `WHERE` clause condition to be applied in-flight: filtering for `is_blocked` when reading the file
          line-by-line.
        - `JOIN` clause condition to be applied in-flight: filtering for `user_id` to be in the set from step 1.
        - Select `transaction_category_id`, and `transaction_amount` only to output required data.
3. Store the map of `transaction_category_id` to cumulative `transaction_amount` and array of _unique_ `user_id` in
   memory as a [`Dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries).
    - Benefits:
        - Minimisation of memory allocation for `transaction_amount`.
        - Minimisation of memory allocation by preserving unique `user_id` only.
    - Requirements:
        - The equivalent of the [query](#query) operation `SUM(t.transaction_amount)` to be applied on the fly.
        - Uniqueness of `user_id` associated with a given category is guaranteed by design of
          the [`Set`](https://docs.python.org/3/tutorial/datastructures.html#sets) data type.
    - Limitations:
        - Array of `user_id` would have to be preserved to keep state of users mapped to a given transaction category to
          execute the equivalent of the [query](#query) operation `COUNT(DISTINCT user_id)`.
4. Calculate the number of unique active users associated with the transaction category.
5. Sort by the total transaction amount.
6. Output.

## Solution Limitations

**The whole set of active users identifiers has to be stored in memory. What if it does not fit?**

It is the problem for large datasets which could be typically resolve either by vertical, or horizontal scaling of
the computation unit.

Vertical scaling is the straightforward approach of increasing the amount of memory, so it fits the amount of data.

Horizontal scaling is achieved by parallel execution of computations following the "map-reduce" logic:

- _Map_:
    - The data are distributed across a cluster of computation nodes according to distribution criteria, e.g. using
      round-robin, or hashing algorithm.
    - The computation for a certain data set's row would be performed on a certain node.
- _Reduce_:
    - Map step's results would be collected on a single node and aggregated to obtain the final result.

Apart from networking and orchestration overhead, the _reduce_ operation could hit the resource bottleneck of the
initial question. In such case, the "map-reduce" process could be repeated, or the resources quota for the "reduce node"
could be raised.   
